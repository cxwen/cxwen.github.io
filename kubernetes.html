<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  kubernetes - cxwen blog
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="cxwen blog" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
 
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:cxwen.github.io ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="https://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; cxwen blog</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="self" href="index.html">Home</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="kubernetes.html">kubernetes</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="15939383708739.html">
                
                  <h1>kubernetes CRD 开发入门指南</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p><img src="https://mmbiz.qpic.cn/mmbiz_png/wmJm3AOn9gsfOibqltsqxvqicLHPiamicYBdeWgmxkJaIoHCRKMMJlUnfU6LK9rbXk9jaSJSd72RhSmaXZgfU7eNBw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""/></p>

<h1 id="toc_0">CRD是什么？</h1>

<p>CRD全称为Custom Resource Definition，是kubernetes提供的开放的扩展api方式。</p>

<p>下面是我的理解：</p>

<p>通常我们说的Operator指的是CRD + Controller。CRD也是kubernetes提供的一种资源类型，我们通过CRD向kubernetes注册自定义的资源类型。空有定义好的CRD没有任何作用，要让自定义的资源类型像kubernetes中的资源一样工作，需要开发一个Controller来控制、调度、实现该资源中定义的状态。而我们真正使用的则是CR(Custom Resource)。</p>

<h3 id="toc_1">举个例子</h3>

<p>现在很多项目都大量使用到CRD, 为了能有更清楚的理解,下面以<a href="https://github.com/cxwen/matrix">matrix项目</a>为例：</p>

<pre><code class="language-text">[root@centos01 ~]# kubectl get crd | grep crd.cxwen.com
dns.crd.cxwen.com                                     2020-06-30T06:21:09Z
etcdclusters.crd.cxwen.com                            2020-06-30T06:21:09Z
masters.crd.cxwen.com                                 2020-06-30T06:21:09Z
matrices.crd.cxwen.com                                2020-06-30T06:21:09Z
networkplugins.crd.cxwen.com                          2020-06-30T06:21:09Z
</code></pre>

<p>可以看到matrix定义了许多的CRD。以<code>masters.crd.cxwen.com</code>为例，来看看CRD里面到底定义了哪些东西。</p>

<pre><code class="language-text">apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: masters.crd.cxwen.com
spec:
  conversion:
    strategy: None
  group: crd.cxwen.com
  names:
    kind: Master
    listKind: MasterList
    plural: masters
    singular: master
  preserveUnknownFields: true
  scope: Namespaced
  versions:
  - additionalPrinterColumns:       # 定义kubectl get时打印出的字段
    - description: version
      jsonPath: .spec.version
      name: VERSION
      type: string
    - description: pod replicas
      jsonPath: .spec.replicas
      name: REPLICAS
      type: string
    - description: etcdcluster name
      jsonPath: .spec.etcdCluster
      name: ETCD
      type: string
    - description: expose type
      jsonPath: .spec.expose.method
      name: EXPOSETYPE
      type: string
    - description: expose node
      jsonPath: .spec.expose.node
      name: EXPOSENODE
      type: string
    - description: expose port
      jsonPath: .spec.expose.port
      name: EXPOSEPORT
      type: string
    - description: phase
      jsonPath: .status.phase
      name: PHASE
      type: string
    - jsonPath: .metadata.creationTimestamp
      name: AGE
      type: date
    name: v1
    schema:                       # 定义CRD的schema
      openAPIV3Schema:
        description: Master is the Schema for the masters API
        properties:
          apiVersion:
            type: string
          kind:
            type: string
          metadata:
            type: object
          spec:
            description: MasterSpec defines the desired state of Master
            properties:
              etcdCluster:
                type: string
              expose:
                properties:
                  method:
                    type: string
                  node:
                    items:
                      type: string
                    type: array
                  port:
                    type: string
                type: object
              imageRegistry:
                type: string
              imageRepo:
                properties:
                  apiserver:
                    type: string
                  controllerManager:
                    type: string
                  proxy:
                    type: string
                  scheduler:
                    type: string
                type: object
              replicas:
                type: integer
              version:
                description: Foo is an example field of Master. Edit Master_types.go
                  to remove/update
                type: string
            type: object
          status:
            description: MasterStatus defines the observed state of Master
            properties:
              adminKubeconfig:
                type: string
              exposeUrl:
                items:
                  type: string
                type: array
              phase:
                description: &#39;INSERT ADDITIONAL STATUS FIELD - define observed state
                  of cluster Important: Run &quot;make&quot; to regenerate code after modifying
                  this file&#39;
                type: string
            type: object
        type: object
    served: true
    storage: true
    subresources:
      status: {}
</code></pre>

<p>CRD里面主要定义了两部分内容：</p>

<p>1、additionalPrinterColumns</p>

<p>顾名思意，<code>additional Printer Columns</code>就是额外的打印列的意思，即设置使用<code>kubectl get</code>命令去查看自定义资源时会打印哪些字段，例如：</p>

<pre><code class="language-text">[root@centos01 ~]# kubectl get master
NAME         VERSION    REPLICAS   ETCD         EXPOSETYPE   EXPOSENODE         EXPOSEPORT   PHASE   AGE
example-km   v1.15.12   1          example-ec   NodePort     [192.168.83.128]   31299        Ready   12m
</code></pre>

<p>这里就打印出了上面CRD中定义的字段。</p>

<p>2、schema</p>

<p>定义Custom Resource的模式或者说规范，这里面定义一个CR的各个属性的数据类型，CR一定要遵循CRD里面的定义才能创建成功。</p>

<p>有以下两种情况：</p>

<ul>
<li>属性是一个基本数据类型</li>
</ul>

<p>type直接指定属性的类型，如<code>apiserver</code>为<code>string</code>类型：</p>

<pre><code class="language-text">apiserver:
  type: string
</code></pre>

<ul>
<li>属性是一个结构体类型</li>
</ul>

<p>type值为object, properties中描述其它属性的类型，如imageRepo属性：</p>

<pre><code class="language-text">  imageRepo:
    type: object
    properties:
      apiserver:
        type: string
      controllerManager:
        type: string
      proxy:
        type: string
      scheduler:
        type: string
</code></pre>

<h3 id="toc_2">总结一下：</h3>

<table>
<thead>
<tr>
<th>类型</th>
<th>说明</th>
</tr>
</thead>

<tbody>
<tr>
<td>Operator</td>
<td>CRD + Controller</td>
</tr>
<tr>
<td>CRD (Custom Resource Definition)</td>
<td>定义自定义资源的各种属性并向kubernetes中注册</td>
</tr>
<tr>
<td>CR (Custom Resource)</td>
<td>通过CRD定义好的真正可以在k8s中使用的资源，类似于pod，deployment这样的k8s中定义好的资源</td>
</tr>
<tr>
<td>Controller</td>
<td>监听CRD的CRUD事件并添加自定义业务逻辑，负责确保其追踪的资源对象的当前状态接近期望状态</td>
</tr>
</tbody>
</table>

<h1 id="toc_3">如何开发？</h1>

<p>了解了CRD的是什么，那如何来开发一个CRD呢？</p>

<p>从上文看起来CRD的定义文件这么长，似乎很复杂。不要怕，这些都可以用工具生成，不需要咱们手动编写的。正所谓工欲善其事，必先利其器。下面就来了解下CRD的开发利器: <a href="https://github.com/kubernetes-sigs/kubebuilder">kubebuilder</a>。有时间的也可以研究下<a href="https://book.kubebuilder.io/">kubebuilder的文档</a>, 里面有详细的介绍。</p>

<h2 id="toc_4">使用kubebuilder构建CRD基本代码框架</h2>

<h3 id="toc_5">环境安装</h3>

<p>可以直接在机器上进行安装，或者使用构建好kubebuilder镜像运行一个容器在容器执行操作：推荐使用容器方式，方便很多。</p>

<h4 id="toc_6">机器直接安装</h4>

<h5 id="toc_7">安装go环境</h5>

<p>首先机器上得安装go环境，不会安装的可以看这个教程：<a href="https://www.runoob.com/go/go-environment.html">Go语言环境安装</a>。 如果因为墙的原因无法从go官网下载安装包，可以访问<a href="https://studygolang.com/dl">go语言中文网</a>进行下载。</p>

<h5 id="toc_8">安装kubebuilder</h5>

<p>接着需要在机器上安装kubebuilder, linux可以使用下面命令安装：</p>

<pre><code class="language-text">wget https://github.com/kubernetes-sigs/kubebuilder/releases/download/v2.3.1/kubebuilder_2.3.1_linux_amd64.tar.gz
tar -zxvf kubebuilder_2.3.1_linux_amd64.tar.gz
cp kubebuilder_2.3.1_linux_amd64/bin/kubebuilder /usr/local/bin/
</code></pre>

<p>其它版本和架构的机器安装方法一样，可以根据需要下载相应的安装包。</p>

<h5 id="toc_9">安装kustomize</h5>

<p><a href="https://github.com/kubernetes-sigs/kustomize">kustomize</a>是一个yaml渲染工具，kubebuilder依赖它进行yaml文件的渲染。</p>

<pre><code class="language-text">curl https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2Fv3.6.1/kustomize_v3.6.1_linux_amd64.tar.gz | tar -zxv -C /usr/local/bin/
</code></pre>

<h4 id="toc_10">容器中运行</h4>

<p>拉取镜像 xwcheng/kubebuilder:2.3.1</p>

<pre><code class="language-text">docker pull xwcheng/kubebuilder:2.3.1
</code></pre>

<p>运行容器</p>

<pre><code class="language-text">docker run -d --name kubebuilder -v /go/src/github.com/cxwen/:/go/src/github.com/cxwen -e GOPATH=/go -e GOROOT=/usr/local/go xwcheng/kubebuilder:2.3.1 sh -c &quot;while true; do sleep 1000000000; done&quot;
</code></pre>

<p>可以将宿主机的目录挂载到容器中。</p>

<p>进入容器</p>

<pre><code class="language-text">docker exec -ti kubebuilder bash
</code></pre>

<h3 id="toc_11">构建代码框架</h3>

<p>1、初始化代码框架</p>

<p>国内因为墙的关系，最好执行 <code>export GOPROXY=https://goproxy.io</code>。</p>

<pre><code class="language-text">mkdir -p crd/
cd crd/
kubebuilder init --domain cxwen.com --license apache2 --owner &quot;cxwen&quot;
</code></pre>

<pre><code class="language-text">Writing scaffold for you to edit...
Get controller runtime:
$ go get sigs.k8s.io/controller-runtime@v0.5.0
go: finding sigs.k8s.io/controller-runtime v0.5.0
......
Update go.mod:
$ go mod tidy
go: downloading github.com/go-logr/zapr v0.1.0
......
Running make:
$ make
go: creating new go.mod: module tmp
go: finding sigs.k8s.io v0.2.5
......
/go/bin/controller-gen object:headerFile=&quot;hack/boilerplate.go.txt&quot; paths=&quot;./...&quot;
go fmt ./...
go vet ./...
go build -o bin/manager main.go
Next: define a resource with:
$ kubebuilder create api
</code></pre>

<p>生成完之后，会出现如下这些文件机目录：</p>

<pre><code class="language-text">[root@centos01 ~]# tree
.
├── bin
│   └── manager
├── config
│   ├── certmanager
│   │   ├── certificate.yaml
│   │   ├── kustomization.yaml
│   │   └── kustomizeconfig.yaml
│   ├── default
│   │   ├── kustomization.yaml
│   │   ├── manager_auth_proxy_patch.yaml
│   │   ├── manager_webhook_patch.yaml
│   │   └── webhookcainjection_patch.yaml
│   ├── manager
│   │   ├── kustomization.yaml
│   │   └── manager.yaml
│   ├── prometheus
│   │   ├── kustomization.yaml
│   │   └── monitor.yaml
│   ├── rbac
│   │   ├── auth_proxy_client_clusterrole.yaml
│   │   ├── auth_proxy_role_binding.yaml
│   │   ├── auth_proxy_role.yaml
│   │   ├── auth_proxy_service.yaml
│   │   ├── kustomization.yaml
│   │   ├── leader_election_role_binding.yaml
│   │   ├── leader_election_role.yaml
│   │   └── role_binding.yaml
│   └── webhook
│       ├── kustomization.yaml
│       ├── kustomizeconfig.yaml
│       └── service.yaml
├── Dockerfile
├── go.mod
├── go.sum
├── hack
│   └── boilerplate.go.txt
├── main.go
├── Makefile
└── PROJECT

9 directories, 30 files
</code></pre>

<p>2、创建CRD</p>

<pre><code class="language-text">[root@centos01 ~]# kubebuilder create api --group crd --version v1 --kind TestCrd
Create Resource [y/n]
y
Create Controller [y/n]
y
Writing scaffold for you to edit...
api/v1/testcrd_types.go
controllers/testcrd_controller.go
Running make:
$ make
/go/bin/controller-gen object:headerFile=&quot;hack/boilerplate.go.txt&quot; paths=&quot;./...&quot;
go fmt ./...
go vet ./...
go build -o bin/manager main.go
</code></pre>

<p>执行完后可以看到目录下出现api和controllers这两个新的目录。</p>

<pre><code class="language-text">[root@centos01 ~]# tree api/
api/
└── v1
    ├── groupversion_info.go
    ├── testcrd_types.go
    └── zz_generated.deepcopy.go

1 directory, 3 files

[root@centos01 ~]# tree controllers/
controllers/
├── suite_test.go
└── testcrd_controller.go

0 directories, 2 files
</code></pre>

<h3 id="toc_12">代码解析</h3>

<h5 id="toc_13">结构体: CRD的血肉</h5>

<p>api目录中存放的是Custom Resource的结构体。 如下所示，TestCrdSpec结构体中可以自定义yaml文件spec属性下需要的字段。TestCrdStatus结构体中可以自定义yaml文件status属性下需要的字段。</p>

<pre><code class="language-golang">// TestCrdSpec defines the desired state of TestCrd
type TestCrdSpec struct {
    // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
    // Important: Run &quot;make&quot; to regenerate code after modifying this file

    // Foo is an example field of TestCrd. Edit TestCrd_types.go to remove/update
    Foo string `json:&quot;foo,omitempty&quot;`
}

// TestCrdStatus defines the observed state of TestCrd
type TestCrdStatus struct {
    // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
    // Important: Run &quot;make&quot; to regenerate code after modifying this file
}

// +kubebuilder:object:root=true

// TestCrd is the Schema for the testcrds API
type TestCrd struct {
    metav1.TypeMeta   `json:&quot;,inline&quot;`
    metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`

    Spec   TestCrdSpec   `json:&quot;spec,omitempty&quot;`
    Status TestCrdStatus `json:&quot;status,omitempty&quot;`
}
</code></pre>

<p>spec和status有什么区别呢？</p>

<p>可以查看kubernetes的官方文档<a href="https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/kubernetes-objects/#%E5%AF%B9%E8%B1%A1%E8%A7%84%E7%BA%A6-spec-%E4%B8%8E%E7%8A%B6%E6%80%81-status">对象规约（Spec）与状态（Status）</a></p>

<blockquote>
<p>每个 Kubernetes 对象包含两个嵌套的对象字段，它们负责管理对象的配置：对象 spec 和 对象 status 。 spec 是必需的，它描述了对象的 期望状态（Desired State） —— 希望对象所具有的特征。 status 描述了对象的 实际状态（Actual State） ，它是由 Kubernetes 系统提供和更新的。在任何时刻，Kubernetes 控制面一直努力地管理着对象的实际状态以与期望状态相匹配。</p>
</blockquote>

<p>对于我们开发的CRD来说，可以像这样理解。</p>

<ul>
<li><p>spec是预先定义的期望状态，也就是控制器 (controller) 里面可以获取到的预置信息，并根据这些信息进行处理调度以达到这个期望状态，并且spec里面的信息不能被控制器里面的代码更改的。</p></li>
<li><p>status里面的字段里面的信息是标志当前实际状态。比如，一个Custom Resource生命周期有三个状态：initializing、ready、teminating, 可以在status中加一个phase字段来表示；当这个CR刚创建时，控制器将phase字段值更新为initializin；CR初始化完成，健康检查等通过，已经可以正常提供服务了，控制器就可以将phase字段置为ready；当这个CR使命已经完成，进入结束阶段，控制器就将phase字段置为teminating, 然后再执行资源清理操作。当然，这些状态的转换，都是需要在控制器代码里来实现的。</p></li>
</ul>

<h5 id="toc_14">Reconcile：CRD的大脑</h5>

<p>如果说结构体是CRD的血肉，那么controller里面的Reconcile方法就是CRD的大脑，因为CRD的所有行为，都是通过这个方法来控制的，这个方法也是我们代码实现的关键所在。每一个CRD都会在controllers目录中生成一个以<code>{CRD名称}_controller.go</code>格式命名的代码文件, Reconcile方法即在这个文件中。</p>

<pre><code class="language-text">func (r *TestCrdReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {
    _ = context.Background()
    _ = r.Log.WithValues(&quot;testcrd&quot;, req.NamespacedName)

    // your logic here

    return ctrl.Result{}, nil
}
</code></pre>

<p>下面是Reconcile实现的一个模板：</p>

<pre><code class="language-text">func (r *TestCrdReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {
    var err error
    ctx := context.Background() // 获取context
    log := r.Log.WithValues(&quot;master&quot;, req.NamespacedName) // 获取日志对象

    log.V(1).Info(&quot;TestCrd reconcile triggering&quot;)
    
    // 从kubernetes中获取crd对象
    testCrd := crdv1.TestCrd{}
    if err = r.Get(ctx, req.NamespacedName, &amp;testCrd); err != nil { 
        if IgnoreNotFound(err) != nil {
            log.Error(err, &quot;unable to fetch testCrd&quot;)
            return ctrl.Result{}, err
        }

        return ctrl.Result{}, nil
    }

    testCrdFinalizer := &quot;crd.cxw.com&quot;
    // 通过DeletionTimestamp字段来判断是删除还是创建更新操作
    if testCrd.ObjectMeta.DeletionTimestamp.IsZero() {
        // 判断是否是创建
        if ! ContainsString(master.ObjectMeta.Finalizers, testCrdFinalizer) {
            testCrd.ObjectMeta.Finalizers = append(testCrd.ObjectMeta.Finalizers, testCrdFinalizer)
            
            // 你的创建处理代码
            
        } else {
            // 你的更新处理代码
        }
    } else {
        if ContainsString(master.ObjectMeta.Finalizers, masterFinalizer) {
            
            // 你的删除处理代码

            testCrd.ObjectMeta.Finalizers = RemoveString(testCrd.ObjectMeta.Finalizers, testCrdFinalizer)
            if err = r.Update(ctx, &amp;testCrd); err != nil {
                return ctrl.Result{}, err
            }
        }
    }

    return ctrl.Result{}, nil
}
</code></pre>

<h5 id="toc_15">注释也是代码</h5>

<p>可以直接在代码里通过格式化的注释来实现授权、添加额外打印字段功能。具体使用可以参考<a href="https://github.com/cxwen/matrix/">matrix</a></p>

<p>1、给控制器授权</p>

<p>在Reconcile方法前面添加, 例如：</p>

<pre><code class="language-text">// +kubebuilder:rbac:groups=crd.cxwen.com,resources=testcrds,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=crd.cxwen.com,resources=testcrds/status,verbs=get;update;patch

func (r *TestCrdReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {
    _ = context.Background()
    _ = r.Log.WithValues(&quot;testcrd&quot;, req.NamespacedName)

    // your logic here

    return ctrl.Result{}, nil
}
</code></pre>

<p>2、添加额外的打印字段</p>

<p>在CRD结构体前面添加，例如：</p>

<pre><code class="language-text">// +kubebuilder:printcolumn:name=&quot;VERSION&quot;,type=&quot;string&quot;,JSONPath=&quot;.spec.version&quot;,description=&quot;version&quot;
// +kubebuilder:printcolumn:name=&quot;REPLICAS&quot;,type=&quot;string&quot;,JSONPath=&quot;.spec.replicas&quot;,description=&quot;pod replicas&quot;
// +kubebuilder:printcolumn:name=&quot;ETCD&quot;,type=&quot;string&quot;,JSONPath=&quot;.spec.etcdCluster&quot;,description=&quot;etcdcluster name&quot;
// +kubebuilder:printcolumn:name=&quot;EXPOSETYPE&quot;,type=&quot;string&quot;,JSONPath=&quot;.spec.expose.method&quot;,description=&quot;expose type&quot;
// +kubebuilder:printcolumn:name=&quot;EXPOSENODE&quot;,type=&quot;string&quot;,JSONPath=&quot;.spec.expose.node&quot;,description=&quot;expose node&quot;
// +kubebuilder:printcolumn:name=&quot;EXPOSEPORT&quot;,type=&quot;string&quot;,JSONPath=&quot;.spec.expose.port&quot;,description=&quot;expose port&quot;
// +kubebuilder:printcolumn:name=&quot;PHASE&quot;,type=&quot;string&quot;,JSONPath=&quot;.status.phase&quot;,description=&quot;phase&quot;
// +kubebuilder:printcolumn:name=&quot;AGE&quot;,type=&quot;date&quot;,JSONPath=&quot;.metadata.creationTimestamp&quot;

// Master is the Schema for the masters API
type Master struct {
    metav1.TypeMeta   `json:&quot;,inline&quot;`
    metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`

    Spec   MasterSpec   `json:&quot;spec,omitempty&quot;`
    Status MasterStatus `json:&quot;status,omitempty&quot;`
}
</code></pre>

<h5 id="toc_16">Finalizer</h5>

<p>Finalizer的作用是基于kubernetes的处理机制。</p>

<p>当api接收到一个资源对象删除操作，Finalizer为空时，kubernetes会直接将这个资源对象删掉，删掉之后，etcd中就不存在改资源对象了，通过api也无法查询到了。</p>

<p>当Finalizer不为空时，kubernetes不会直接删除改资源对象，而是在对象ObjectMeta中添加DeletionTimestamp这么个字段，一直要等到Finalizer为空时才把资源对象删除。</p>

<p>所以，当我们的CRD在删除时有需要进行资源清理操作时，就可以在创建时添加上Finalizer，当检测到DeletionTimestamp不为空时，就知道该资源对象处于删除状态了，然后执行资源清理操作，最后移除Finalizer即可。</p>

<p>Finalizer的添加很简单，它的值可以是任何字符串。当然，为了起到一些标志作用，可以使用有意义的字符串。</p>

<h5 id="toc_17">OwnerReference</h5>

<p>kubernetes GC在删除一个对象时，任何 ownerReference 是该对象的对象都会被清除。</p>

<p>下面是kubernetes官方文档中的描述。</p>

<blockquote>
<p>某些 Kubernetes 对象是其它一些对象的所有者。例如，一个 ReplicaSet 是一组 Pod 的所有者。 具有所有者的对象被称为是所有者的 附属 。 每个附属对象具有一个指向其所属对象的 metadata.ownerReferences 字段。<br/>
有时，Kubernetes 会自动设置 ownerReference 的值。 例如，当创建一个 ReplicaSet 时，Kubernetes 自动设置 ReplicaSet 中每个 Pod 的 ownerReference 字段值。 在 Kubernetes 1.8 版本，Kubernetes 会自动为某些对象设置 ownerReference 的值，这些对象是由 ReplicationController、ReplicaSet、StatefulSet、DaemonSet、Deployment、Job 和 CronJob 所创建或管理。 也可以通过手动设置 ownerReference 的值，来指定所有者和附属之间的关系。</p>
</blockquote>

<p>添加OwnerReference, 以deployment为例,在ObjectMeta下面添加OwnerReferences即可：</p>

<pre><code class="language-text">Deployment{
    TypeMeta: metav1.TypeMeta{
        APIVersion: &quot;apps/v1&quot;,
        Kind:       &quot;Deployment&quot;,
    },
    ObjectMeta: metav1.ObjectMeta{
        Name:      test,
        Namespace: test,
        OwnerReferences: []metav1.OwnerReference{
            *metav1.NewControllerRef(app, schema.GroupVersionKind{
                Group: v1.SchemeGroupVersion.Group,
                Version: v1.SchemeGroupVersion.Version,
                Kind: &quot;TestCrd&quot;,
            }),
        },
    },
    ......
}

</code></pre>

<h5 id="toc_18">小结</h5>

<p>进行CRD开发时</p>

<ul>
<li>使用注释的方法来为controller配置权限，以及添加额外打印字段</li>
<li>使用Finalizer来做资源的清理</li>
<li>使用OwnerReference进行对象之间依赖关系的管理</li>
</ul>

<p>其它代码开发技巧可以研究下<a href="https://book.kubebuilder.io/">kubebuilder的文档</a>。</p>

<h2 id="toc_19">源码小窥</h2>

<p>kubebuilder生成的源码架构，主要是基于<a href="https://github.com/kubernetes-sigs/controller-runtime">controller-runtime</a>这个go代码库。这个代码库对controller操作做了很好的封装，基于它开发CRD非常方便。</p>

<h3 id="toc_20">main.go</h3>

<pre><code class="language-text">var (
    scheme   = runtime.NewScheme()
    setupLog = ctrl.Log.WithName(&quot;setup&quot;)
)

// 注册scheme
func init() {
    _ = clientgoscheme.AddToScheme(scheme)

    _ = crdv1.AddToScheme(scheme)
    // +kubebuilder:scaffold:scheme
}

func main() {
    ......
    
    // 初始化manager
    mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{
        Scheme:             scheme,
        MetricsBindAddress: metricsAddr,
        Port:               9443,
        LeaderElection:     enableLeaderElection,
        LeaderElectionID:   &quot;6ed5364d.cxwen.com&quot;,
    })
    if err != nil {
        setupLog.Error(err, &quot;unable to start manager&quot;)
        os.Exit(1)
    }

    // 初始化controller
    if err = (&amp;controllers.TestCrdReconciler{
        Client: mgr.GetClient(),
        Log:    ctrl.Log.WithName(&quot;controllers&quot;).WithName(&quot;TestCrd&quot;),
        Scheme: mgr.GetScheme(),
    }).SetupWithManager(mgr); err != nil {
        setupLog.Error(err, &quot;unable to create controller&quot;, &quot;controller&quot;, &quot;TestCrd&quot;)
        os.Exit(1)
    }
    // +kubebuilder:scaffold:builder

    // 启动manager
    setupLog.Info(&quot;starting manager&quot;)
    if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {
        setupLog.Error(err, &quot;problem running manager&quot;)
        os.Exit(1)
    }
}
</code></pre>

<p>main.go中主要做了下面两件事：</p>

<h5 id="toc_21">1、注册scheme</h5>

<p>在controller中，如果需要使用manager提供的client操作某种类型的资源，需要将资源类型注册到scheme中。从代码中的init函数可以看到，crdv1中的类型被注册到了scheme中。</p>

<h5 id="toc_22">2、创建并启动manager</h5>

<p>这个manager就是管理controller的manager, 一个manager中可以管理多个controller。</p>

<p>创建manager过程中，创建了两个很重要的对象：cache和client。这两个对象是manager中所有controller共享的。</p>

<p>下面是创建Manager的函数：</p>

<pre><code class="language-text">// New returns a new Manager for creating Controllers.
func New(config *rest.Config, options Options) (Manager, error) {
    // Initialize a rest.config if none was specified
    if config == nil {
        return nil, fmt.Errorf(&quot;must specify Config&quot;)
    }

    // Set default values for options fields
    options = setOptionsDefaults(options)

    // Create the mapper provider
    mapper, err := options.MapperProvider(config)
    if err != nil {
        log.Error(err, &quot;Failed to get API Group-Resources&quot;)
        return nil, err
    }

    // Create the cache for the cached read client and registering informers
    cache, err := options.NewCache(config, cache.Options{Scheme: options.Scheme, Mapper: mapper, Resync: options.SyncPeriod, Namespace: options.Namespace})
    if err != nil {
        return nil, err
    }

    apiReader, err := client.New(config, client.Options{Scheme: options.Scheme, Mapper: mapper})
    if err != nil {
        return nil, err
    }

    writeObj, err := options.NewClient(cache, config, client.Options{Scheme: options.Scheme, Mapper: mapper})
    if err != nil {
        return nil, err
    }
    // Create the recorder provider to inject event recorders for the components.
    // TODO(directxman12): the log for the event provider should have a context (name, tags, etc) specific
    // to the particular controller that it&#39;s being injected into, rather than a generic one like is here.
    recorderProvider, err := options.newRecorderProvider(config, options.Scheme, log.WithName(&quot;events&quot;), options.EventBroadcaster)
    if err != nil {
        return nil, err
    }

    // Create the resource lock to enable leader election)
    resourceLock, err := options.newResourceLock(config, recorderProvider, leaderelection.Options{
        LeaderElection:          options.LeaderElection,
        LeaderElectionID:        options.LeaderElectionID,
        LeaderElectionNamespace: options.LeaderElectionNamespace,
    })
    if err != nil {
        return nil, err
    }

    // Create the metrics listener. This will throw an error if the metrics bind
    // address is invalid or already in use.
    metricsListener, err := options.newMetricsListener(options.MetricsBindAddress)
    if err != nil {
        return nil, err
    }

    // Create health probes listener. This will throw an error if the bind
    // address is invalid or already in use.
    healthProbeListener, err := options.newHealthProbeListener(options.HealthProbeBindAddress)
    if err != nil {
        return nil, err
    }

    stop := make(chan struct{})

    return &amp;controllerManager{
        config:                config,
        scheme:                options.Scheme,
        cache:                 cache,
        fieldIndexes:          cache,
        client:                writeObj,
        apiReader:             apiReader,
        recorderProvider:      recorderProvider,
        resourceLock:          resourceLock,
        mapper:                mapper,
        metricsListener:       metricsListener,
        internalStop:          stop,
        internalStopper:       stop,
        port:                  options.Port,
        host:                  options.Host,
        certDir:               options.CertDir,
        leaseDuration:         *options.LeaseDuration,
        renewDeadline:         *options.RenewDeadline,
        retryPeriod:           *options.RetryPeriod,
        healthProbeListener:   healthProbeListener,
        readinessEndpointName: options.ReadinessEndpointName,
        livenessEndpointName:  options.LivenessEndpointName,
    }, nil
}
</code></pre>

<ul>
<li>cache</li>
</ul>

<p>cache用到了kubernetes中一个重要的工具包：informer。</p>

<p>cache主要就是创建了InformersMap，scheme里面的每个GVK (GroupVersionKind结构体，包含Group、Version、Kind三个字段，可以唯一确定一个资源) 都创建了对应的 Informer，通过 informersByGVK这个map来存放GVK和Informer的映射关系，每个 Informer会根据ListWatch 函数对对应的GVK进行List和Watch。我们为controller开发的Reconcile方法最终都会注册到informer的handler中，这样利用informer就可以达到监控资源的事件并触发Reconcile目的。</p>

<pre><code class="language-text">// newSpecificInformersMap returns a new specificInformersMap (like
// the generical InformersMap, except that it doesn&#39;t implement WaitForCacheSync).
func newSpecificInformersMap(config *rest.Config,
    scheme *runtime.Scheme,
    mapper meta.RESTMapper,
    resync time.Duration,
    namespace string,
    createListWatcher createListWatcherFunc) *specificInformersMap {
    ip := &amp;specificInformersMap{
        config:            config,
        Scheme:            scheme,
        mapper:            mapper,
        informersByGVK:    make(map[schema.GroupVersionKind]*MapEntry), // schema GVK和informer映射
        codecs:            serializer.NewCodecFactory(scheme),
        paramCodec:        runtime.NewParameterCodec(scheme),
        resync:            resync,
        startWait:         make(chan struct{}),
        createListWatcher: createListWatcher,
        namespace:         namespace,
    }
    return ip
}
</code></pre>

<p>MapEntry中包含最终创建的informer对象。</p>

<pre><code class="language-text">// MapEntry contains the cached data for an Informer
type MapEntry struct {
    // Informer is the cached informer
    Informer cache.SharedIndexInformer

    // CacheReader wraps Informer and implements the CacheReader interface for a single type
    Reader CacheReader
}
</code></pre>

<ul>
<li>client</li>
</ul>

<p>从下面的代码可以看出，读操作使用上面创建的 cache，写操作使用client直连kubernetes。</p>

<pre><code class="language-text">// defaultNewClient creates the default caching client
func defaultNewClient(cache cache.Cache, config *rest.Config, options client.Options) (client.Client, error) {
    // Create the Client for Write operations.
    c, err := client.New(config, options)
    if err != nil {
        return nil, err
    }

    return &amp;client.DelegatingClient{
        Reader: &amp;client.DelegatingReader{
            CacheReader:  cache,
            ClientReader: c,
        },
        Writer:       c,
        StatusClient: c,
    }, nil
}
</code></pre>

<h3 id="toc_23">CRD Reconcile执行</h3>

<p>在main.go中, 通过下面的代码把Manager中的client、scheme以及日志对象传递给相应的CRD对象。</p>

<pre><code class="language-text">if err = (&amp;controllers.TestCrdReconciler{
    Client: mgr.GetClient(),
    Log:    ctrl.Log.WithName(&quot;controllers&quot;).WithName(&quot;TestCrd&quot;),
    Scheme: mgr.GetScheme(),
}).SetupWithManager(mgr); err != nil {
    setupLog.Error(err, &quot;unable to create controller&quot;, &quot;controller&quot;, &quot;TestCrd&quot;)
    os.Exit(1)
}
</code></pre>

<pre><code class="language-text">// TestCrdReconciler reconciles a TestCrd object
type TestCrdReconciler struct {
    client.Client
    Log    logr.Logger
    Scheme *runtime.Scheme
}
</code></pre>

<p>然后在Reconcile方法中就可以直接使用client来进行CURD操作。</p>

<h2 id="toc_24">总结</h2>

<p>kubernetes的强大之处之一就是支持CRD对API进行扩展，当今很多项目都大量使用到CRD，像calico、istio以及kubevirt等等。如果有在kubernetes上层进行二次开发需求，可以优先考虑CRD,这是一种非常优雅的扩展方式，也是kubernetes生态的发展趋势。除此之外，有kubebuilder这个CRD开发利器，也能让我们的开发工作事半功倍。</p>

<h5 id="toc_25">资源链接</h5>

<blockquote>
<p>matrix github：<a href="https://github.com/cxwen/matrix">https://github.com/cxwen/matrix</a><br/><br/>
kubebuilder github: <a href="https://github.com/kubernetes-sigs/kubebuilder">https://github.com/kubernetes-sigs/kubebuilder</a><br/>
kubebuilder的文档: <a href="https://book.kubebuilder.io/">https://book.kubebuilder.io/</a><br/><br/>
Go语言环境安装: <a href="https://www.runoob.com/go/go-environment.html">https://www.runoob.com/go/go-environment.html</a><br/><br/>
go语言中文网go安装包下载: <a href="https://studygolang.com/dl">https://studygolang.com/dl</a><br/><br/>
kustomize github: <a href="https://github.com/kubernetes-sigs/kustomize">https://github.com/kubernetes-sigs/kustomize</a><br/><br/>
kubernetes的官方文档对象规约（Spec）与状态（Status）: <a href="https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/kubernetes-objects/#%E5%AF%B9%E8%B1%A1%E8%A7%84%E7%BA%A6-spec-%E4%B8%8E%E7%8A%B6%E6%80%81-status">https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/kubernetes-objects/#%E5%AF%B9%E8%B1%A1%E8%A7%84%E7%BA%A6-spec-%E4%B8%8E%E7%8A%B6%E6%80%81-status</a><br/><br/>
controller-runtime github: <a href="https://github.com/kubernetes-sigs/controller-runtime">https://github.com/kubernetes-sigs/controller-runtime</a></p>
</blockquote>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/wmJm3AOn9gsfOibqltsqxvqicLHPiamicYBdK3arv8jUGGOFVdFnDTOsY5QmOrdS7EMibDW2zrpwc2MuH5niaic21Jh6A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2020/07/05</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='kubernetes.html'>kubernetes</a></span>
          				   
                    

                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15939392688573.html">
                
                  <h1>kubernetes v1.10.0 高可用集群部署</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>kubernetes官方并为明确给出高可用生产集群的部署方案，经过调研，使用keepalived和haproxy可以实现高可用集群的部署。该方案也已经过实践检验，运行的还是比较稳定的。</p>

<h2 id="toc_0">机器</h2>

<table>
<thead>
<tr>
<th>IP</th>
<th>用途</th>
<th>备注</th>
</tr>
</thead>

<tbody>
<tr>
<td>172.28.79.11</td>
<td>master、etcd</td>
<td>主节点</td>
</tr>
<tr>
<td>172.28.79.12</td>
<td>master、etcd、keepalived、haproxy</td>
<td>主节点，同时部署keepalived、haproxy，保证master高可用</td>
</tr>
<tr>
<td>172.28.79.13</td>
<td>master、etcd、keepalived、haproxy</td>
<td>主节点，同时部署keepalived、haproxy，保证master高可用</td>
</tr>
<tr>
<td>172.28.79.14</td>
<td>node、etcd</td>
<td>业务节点</td>
</tr>
<tr>
<td>172.28.79.15</td>
<td>node、etcd</td>
<td>业务节点</td>
</tr>
<tr>
<td>172.28.79.16</td>
<td>node</td>
<td>业务节点</td>
</tr>
<tr>
<td>172.28.79.17</td>
<td>node</td>
<td>业务节点</td>
</tr>
<tr>
<td>172.28.79.18</td>
<td>node</td>
<td>业务节点</td>
</tr>
<tr>
<td>172.28.79.19</td>
<td>node</td>
<td>业务节点</td>
</tr>
<tr>
<td>172.28.79.20</td>
<td>node</td>
<td>业务节点</td>
</tr>
<tr>
<td>172.28.79.21</td>
<td>node</td>
<td>业务节点</td>
</tr>
<tr>
<td>172.28.79.22</td>
<td>node</td>
<td>业务节点</td>
</tr>
<tr>
<td>172.28.79.23</td>
<td>node</td>
<td>业务节点</td>
</tr>
<tr>
<td>172.28.79.24</td>
<td>node、harbor</td>
<td>业务节点</td>
</tr>
<tr>
<td>172.28.79.25</td>
<td>node</td>
<td>业务节点</td>
</tr>
</tbody>
</table>

<h1 id="toc_1">机器基础配置信息</h1>

<h2 id="toc_2">版本信息</h2>

<table>
<thead>
<tr>
<th>项目</th>
<th>版本</th>
</tr>
</thead>

<tbody>
<tr>
<td>系统版本</td>
<td>CentOS Linux release 7.4.1708 (Core)</td>
</tr>
<tr>
<td>内核版本</td>
<td>4.14.49</td>
</tr>
</tbody>
</table>

<h2 id="toc_3">ntpd时间同步配置</h2>

<p>/etc/ntp.conf</p>

<pre><code class="language-shell">#perfer 表示『优先使用』的服务器
server ntp.aliyun.com prefer
#下面没有prefer参数，做为备用NTP时钟上层服务器地址，我这里设置的是公网，语音云则可以设置其他两地NTP IP。
server cn.ntp.org.cn
#我们每一个system clock的频率都有小小的误差,这个就是为什么机器运行一段时间后会不精确. NTP会自动来监测我们时钟的误差值并予以调整.但问题是这是一个冗长的过程,所以它会把记录下来的误差先写入driftfile.这样即使你重新开机以后之前的计算结果也就不会丢了
driftfile /var/lib/ntp/ntp.drift
statistics loopstats peerstats clockstats
filegen loopstats file loopstats type day enable
filegen peerstats file peerstats type day enable
filegen clockstats file clockstats type day enable
# By default, exchange time with everybody, but don&#39;t allow configuration.
restrict -4 default kod notrap nomodify nopeer noquery
restrict -6 default kod notrap nomodify nopeer noquery
# Local users may interrogate the ntp server more closely.
restrict 127.0.0.1
restrict ::1
</code></pre>

<h1 id="toc_4">kubernetes组件配置信息</h1>

<h2 id="toc_5">组件版本</h2>

<table>
<thead>
<tr>
<th>组件名</th>
<th>版本</th>
</tr>
</thead>

<tbody>
<tr>
<td>docker</td>
<td>Docker version 1.12.6, build 78d1802</td>
</tr>
<tr>
<td>kubernetes</td>
<td>v1.10.0</td>
</tr>
<tr>
<td>etcd</td>
<td>3.1.12</td>
</tr>
<tr>
<td>calico</td>
<td>v3.0.4</td>
</tr>
<tr>
<td>harbor</td>
<td>v1.2.0</td>
</tr>
<tr>
<td>keepalived</td>
<td>v1.3.5</td>
</tr>
<tr>
<td>haproxy</td>
<td>1.7</td>
</tr>
</tbody>
</table>

<h2 id="toc_6">配置</h2>

<p>组件配置</p>

<h3 id="toc_7">docker</h3>

<p>配置文件：/usr/lib/systemd/system/docker.service</p>

<pre><code class="language-shell">[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target

[Service]
Type=notify
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker
ExecStart=/usr/bin/dockerd -H 0.0.0.0:2375 -H unix:///var/run/docker.sock --registry-mirror https://registry.docker-cn.com --insecure-registry 172.16.59.153 --insecure-registry hub.cxwen.cn --insecure-registry k8s.gcr.io --insecure-registry quay.io --default-ulimit core=0:0 --live-restore
ExecReload=/bin/kill -s HUP $MAINPID
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
#TasksMax=infinity
TimeoutStartSec=0
# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes
# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
</code></pre>

<pre><code class="language-shell">--registry-mirror：指定 docker pull 时使用的注册服务器镜像地址,指定为https://registry.docker-cn.com可以加快docker hub中的镜像拉取速度
--insecure-registry：配置非安全的docker镜像注册服务器
--default-ulimit：配置容器默认的ulimit选项
--live-restore：开启此选项，当dockerd服务出现问题时，容器照样运行，服务恢复后，容器也可以再被服务抓到并可管理
</code></pre>

<h3 id="toc_8">kubernetes</h3>

<h4 id="toc_9">etcd</h4>

<p>以172.28.79.11节点为例，其它节点类似：</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  labels:
    component: etcd
    tier: control-plane
  name: etcd-172.28.79.11
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --name=infra0
    - --initial-advertise-peer-urls=http://172.28.79.11:2380
    - --listen-peer-urls=http://172.28.79.11:2380
    - --listen-client-urls=http://172.28.79.11:2379,http://127.0.0.1:2379
    - --advertise-client-urls=http://172.28.79.11:2379
    - --data-dir=/var/lib/etcd
    - --initial-cluster-token=etcd-cluster-1
    - --initial-cluster=infra0=http://172.28.79.11:2380,infra1=http://172.28.79.12:2380,infra2=http://172.28.79.13:2380,infra3=http://172.28.79.14:2380,infra4=http://172.28.79.15:2380
    - --initial-cluster-state=new
    image: k8s.gcr.io/etcd-amd64:3.1.12
    livenessProbe:
      httpGet:
        host: 127.0.0.1
        path: /health
        port: 2379
        scheme: HTTP
      failureThreshold: 8
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: etcd
    volumeMounts:
    - name: etcd-data
      mountPath: /var/lib/etcd
  hostNetwork: true
  volumes:
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
</code></pre>

<h3 id="toc_10">kubernetes系统组件</h3>

<h4 id="toc_11">kubeadm init 启动k8s集群config.yaml配置</h4>

<pre><code class="language-yaml">apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
networking:
  podSubnet: 192.168.0.0/16
api:
  advertiseAddress: 172.28.79.11
etcd:
  endpoints:
  - http://172.28.79.11:2379 
  - http://172.28.79.12:2379
  - http://172.28.79.13:2379
  - http://172.28.79.14:2379
  - http://172.28.79.15:2379

apiServerCertSANs:
  - 172.28.79.11
  - master01.bja.paas
  - 172.28.79.12
  - master02.bja.paas
  - 172.28.79.13
  - master03.bja.paas
  - 172.28.79.10
  
  - 127.0.0.1
token:
kubernetesVersion: v1.10.0
apiServerExtraArgs:
  endpoint-reconciler-type: lease
  bind-address: 172.28.79.11
  runtime-config: storage.k8s.io/v1alpha1=true
  admission-control: NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota
featureGates:
  CoreDNS: true
</code></pre>

<h4 id="toc_12">kubelet配置</h4>

<p>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</p>

<pre><code class="language-shell">[Service]
Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot;
Environment=&quot;KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true&quot;
Environment=&quot;KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin&quot;
Environment=&quot;KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local&quot;
Environment=&quot;KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt&quot;
Environment=&quot;KUBELET_CADVISOR_ARGS=--cadvisor-port=0&quot;
Environment=&quot;KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs&quot;
Environment=&quot;KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki --eviction-hard=memory.available&lt;5%,nodefs.available&lt;5%,imagefs.available&lt;5%&quot;
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CGROUP_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS
</code></pre>

<h3 id="toc_13">keepalived</h3>

<p>keepalived采取直接在物理机部署，使用yum命令进行安装，并设置开机自启。</p>

<p>yum install -y keepalived<br/><br/>
systemctl enable keeplalived  </p>

<h4 id="toc_14">配置keepalived配置文件</h4>

<p>启动配置文件：&quot;/etc/keepalived/keepalived.conf&quot;。keepalived的MASTER和BACKUP配置有部分差异</p>

<p>MASTER</p>

<pre><code class="language-shell">! Configuration File for keepalived

global_defs {
    notification_email {
      root@localhost
    }
    router_id master02
}

vrrp_script chk_haproxy {
    script &quot;/etc/keepalived/haproxy_check.sh&quot;
    interval 3
    weight -20
}

vrrp_instance VI_1 {
    state MASTER    # BACKUP节点改成BACKUP
    interface bond1
    virtual_router_id 151
    priority 110    # BACKUP节点改成100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
       172.28.79.10 # k8s使用的VIP
       172.28.79.9  # 数据库组件使用的VIP
    }
    track_script {
       chk_haproxy
    }
}
</code></pre>

<p>haproxy检查脚本：/etc/keepalived/haproxy_check.sh</p>

<pre><code class="language-shell">#!/bin/bash

if [ `ps -C haproxy --no-header |wc -l` -eq 0 ] ; then
    docker restart k8s-haproxy
    sleep 2
    if [ `ps -C haproxy --no-header |wc -l` -eq 0 ] ; then
        service keepalived stop
    fi
fi
</code></pre>

<h3 id="toc_15">haproxy</h3>

<p>haproxy以容器的形式启动，启动命令如下：</p>

<pre><code class="language-shell">docker run -d --net host --restart always --name k8s-haproxy -v /etc/haproxy:/usr/local/etc/haproxy:ro hub.xfyun.cn/k8s/haproxy:1.7
</code></pre>

<p>haproxy配置文件：/etc/haproxy/haproxy.cfg</p>

<pre><code class="language-shell">global
  daemon
  log 127.0.0.1 local0
  log 127.0.0.1 local1 notice
  maxconn 4096

defaults
  log               global
  retries           3
  maxconn           2000
  timeout connect   5s
  timeout client    50s
  timeout server    50s

frontend k8s
  bind *:6444
  mode tcp
  default_backend k8s-backend

backend k8s-backend
  balance roundrobin
  mode tcp
  server k8s-1 172.28.79.11:6443 check
  server k8s-2 172.28.79.12:6443 check
  server k8s-3 172.28.79.13:6443 check
</code></pre>

<h2 id="toc_16">部署完成后操作</h2>

<h3 id="toc_17">修改kube-proxy configmap</h3>

<pre><code class="language-shell">kubectl edit configmap kube-proxy -n kube-system
</code></pre>

<pre><code class="language-yaml">.....
kubeconfig.conf: |-
  apiVersion: v1
  kind: Config
  clusters:
  - cluster:
      certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      server: https://172.28.79.10:6444  # 更改此行ip为vip,改成172.28.79.10
    name: default
  contexts:
  - context:
      cluster: default
      namespace: default
      user: default
    name: default
  current-context: default
  users:
  - name: default
    user:
      tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
......
</code></pre>

<p>执行如下命令让kube-proxy组件重新启动</p>

<pre><code class="language-shell">kubectl get pod -n kube-system | grep kube-proxy | awk &#39;{print $1}&#39; | xargs kubectl delete pod -n kube-system
</code></pre>

<h3 id="toc_18">修改所有node节点kubelet.conf</h3>

<pre><code class="language-shell">/etc/kubernetes/kubelet.conf
</code></pre>

<pre><code class="language-yaml">apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1EVXhPREF4TXpNME1Gb1hEVEk0TURVeE5UQXhNek0wTUZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTGJoCmw1TDRaNHFiWTJ3MmY5TFlEb0ZqVlhhcHRhYklkQmZmTS9zMTJaWFd1NU5LYWlPR09ub3RxK1gwM0VJb3Z4VEkKUGh5NzBqY294VGlLUTk5ZkFsUS82a2Vhc0x5MDNGZXJvYkhmaldUenBkZE5mWVNEZStMazlMV0hIZ0phOXVUQQpDU3kyay9sZGo3VWQ0Sk9pMi9lcGhVTUNNMUNlbmdPeWZDNUl0SUpFZzJmMk95cTE5U0JBeW1zYzFTalg5Q0F6CnNyMlhiTm9hK1lVS2Flek1QSldvYlNxdEg0czQ1TkluYytMREJFTkk4VGVITktybENsamdIeUorUjU1V2pCTW8KeSs3Y1BxL2cwTkxmSU4xRjJVbkFFa3RTSmVYUFBSaGlQUUhJcGRBU0xySXhVcE9HNlN3Yk51bmRGdGsxaUJiUgpUSW9md2UyT0VhZkhySmV5OHJrQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLME1mOFM5VjUyaG9VZ3JYcGQyK09rOHF2Ny8KR3hpWnRFelFISW9RdWRLLzJ2ZGJHbXdnem10V3hFNHRYRWQyUnlXTTZZR1VQSmNpMmszY1Z6QkpSaGcvWFB2UQppRVBpUDk5ZkdiM0kxd0QyanlURWVaZVd1ekdSRDk5ait3bStmcE9wQzB2ZU1LN3hzM1VURjRFOFlhWGcwNmdDCjBXTkFNdTRxQmZaSUlKSEVDVDhLUlB5TEN5Zlgvbm84Q25WTndaM3pCbGZaQmFONGZaOWw0UUdGMVd4dlc0OHkKYmpvRDhqUVJnL1kwYUVUMWMrSEhpWTNmNDF0dG9kMWJoSWR3c1NDNUhhRjJQSVAvZ2dCSnZ2Uzh2V1cwcVRDegpDV2EzcVJ0bVB0MHdtcEZic2RPWmdsWkl6aWduYTdaaDFWMDJVM0VFZ2kwYjNGZWR5OW5MRUZaMGJZbz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://172.28.79.10:6444   # 此处改为VIP加haproxy监听端口6444
  name: default-cluster
contexts:
- context:
    cluster: default-cluster
    namespace: default
    user: default-auth
  name: default-context
current-context: default-context
kind: Config
preferences: {}
users:
- name: default-auth
  user:
    client-certificate: /var/lib/kubelet/pki/kubelet-client.crt
    client-key: /var/lib/kubelet/pki/kubelet-client.key
</code></pre>

<h2 id="toc_19">部署前注意事项</h2>

<h3 id="toc_20">1. 确保所有节点时间同步</h3>

<p>使用ntpdate命令进行时间同步，若无私网时间服务器，可以使用阿里云时间服务器。</p>

<pre><code class="language-shell">ntpdate ntp1.aliyun.com
</code></pre>

<h3 id="toc_21">2. 确保所有节点ip转发功能打开</h3>

<pre><code class="language-shell">net.ipv4.ip_forward = 1
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2020/07/05</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='kubernetes.html'>kubernetes</a></span>
          				   
                    

                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <h1>cxwen blog</h1>
                <div class="site-des">分享个人技术经验、心得体会：kubernetes, docker, 容器，虚拟化，网络等</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/cxwen" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:cxwzero@hotmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="kubernetes.html"><strong>kubernetes</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15939383708739.html">kubernetes CRD 开发入门指南</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15939392688573.html">kubernetes v1.10.0 高可用集群部署</a>
			      </li>
		     
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>



  














<style type="text/css">
figure{margin: 0;padding: 0;}
figcaption{text-align:center;}

/* PrismJS 1.14.0
 http://prismjs.com/download.html#themes=prism&languages=markup+css+clike+javascript */
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */

code[class*="language-"],
pre[class*="language-"] {
    color: black;
    background: none;
    text-shadow: 0 1px white;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
    
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
    
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
    text-shadow: none;
    background:#b3d4fc;
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
    text-shadow: none;
    background: #b3d4fc;
}

@media print {
    code[class*="language-"],
    pre[class*="language-"] {
        text-shadow: none;
    }
}

/* Code blocks */
pre[class*="language-"] {
    padding: 1em;
    margin: .5em 0;
    overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
    background: #F7F7F7;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
    padding: .1em;
    border-radius: .3em;
    white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
    color: slategray;
}

.token.punctuation {
    color: #999;
}

.namespace {
    opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
    color: #905;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
    color: #690;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
    color: #9a6e3a;
    background: hsla(0, 0%, 100%, .5);
}

.token.atrule,
.token.attr-value,
.token.keyword {
    color: #07a;
}

.token.function,
.token.class-name {
    color: #DD4A68;
}

.token.regex,
.token.important,
.token.variable {
    color: #e90;
}

.token.important,
.token.bold {
    font-weight: bold;
}
.token.italic {
    font-style: italic;
}

.token.entity {
    cursor: help;
}


pre[class*="language-"].line-numbers {
    position: relative;
    padding-left: 3.8em;
    counter-reset: linenumber;
}

pre[class*="language-"].line-numbers > code {
    position: relative;
    white-space: inherit;
}

.line-numbers .line-numbers-rows {
    position: absolute;
    pointer-events: none;
    top: 0;
    font-size: 100%;
    left: -3.8em;
    width: 3em; /* works for line-numbers below 1000 lines */
    letter-spacing: -1px;
    border-right: 1px solid #999;

    -webkit-user-select: none;
    -moz-user-select: none;
    -ms-user-select: none;
    user-select: none;

}

    .line-numbers-rows > span {
        pointer-events: none;
        display: block;
        counter-increment: linenumber;
    }

        .line-numbers-rows > span:before {
            content: counter(linenumber);
            color: #999;
            display: block;
            padding-right: 0.8em;
            text-align: right;
        }

</style>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>



  </body>
</html>
