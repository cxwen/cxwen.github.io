{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"John Doe","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"kubernetes v1.10.0 高可用集群部署","slug":"kubernetes v1.10.0 高可用集群部署","date":"2019-06-20T13:28:14.977Z","updated":"2019-06-20T13:28:14.978Z","comments":true,"path":"2019/06/20/kubernetes v1.10.0 高可用集群部署/","link":"","permalink":"http://yoursite.com/2019/06/20/kubernetes v1.10.0 高可用集群部署/","excerpt":"","text":"kubernetes v1.10.0 高可用集群部署kubernetes官方并为明确给出高可用生产集群的部署方案，经过调研，使用keepalived和haproxy可以实现高可用集群的部署。该方案也已经过实践检验，运行的还是比较稳定的。 机器 IP 用途 备注 172.28.79.11 master、etcd 主节点 172.28.79.12 master、etcd、keepalived、haproxy 主节点，同时部署keepalived、haproxy，保证master高可用 172.28.79.13 master、etcd、keepalived、haproxy 主节点，同时部署keepalived、haproxy，保证master高可用 172.28.79.14 node、etcd 业务节点 172.28.79.15 node、etcd 业务节点 172.28.79.16 node 业务节点 172.28.79.17 node 业务节点 172.28.79.18 node 业务节点 172.28.79.19 node 业务节点 172.28.79.20 node 业务节点 172.28.79.21 node 业务节点 172.28.79.22 node 业务节点 172.28.79.23 node 业务节点 172.28.79.24 node、harbor 业务节点 172.28.79.25 node 业务节点 机器基础配置信息版本信息 项目 版本 系统版本 CentOS Linux release 7.4.1708 (Core) 内核版本 4.14.49 ntpd时间同步配置/etc/ntp.conf 12345678910111213141516#perfer 表示『优先使用』的服务器server ntp.aliyun.com prefer#下面没有prefer参数，做为备用NTP时钟上层服务器地址，我这里设置的是公网，语音云则可以设置其他两地NTP IP。server cn.ntp.org.cn#我们每一个system clock的频率都有小小的误差,这个就是为什么机器运行一段时间后会不精确. NTP会自动来监测我们时钟的误差值并予以调整.但问题是这是一个冗长的过程,所以它会把记录下来的误差先写入driftfile.这样即使你重新开机以后之前的计算结果也就不会丢了driftfile /var/lib/ntp/ntp.driftstatistics loopstats peerstats clockstatsfilegen loopstats file loopstats type day enablefilegen peerstats file peerstats type day enablefilegen clockstats file clockstats type day enable# By default, exchange time with everybody, but don't allow configuration.restrict -4 default kod notrap nomodify nopeer noqueryrestrict -6 default kod notrap nomodify nopeer noquery# Local users may interrogate the ntp server more closely.restrict 127.0.0.1restrict ::1 kubernetes组件配置信息组件版本 组件名 版本 docker Docker version 1.12.6, build 78d1802 kubernetes v1.10.0 etcd 3.1.12 calico v3.0.4 harbor v1.2.0 keepalived v1.3.5 haproxy 1.7 配置组件配置 docker配置文件：/usr/lib/systemd/system/docker.service 12345678910111213141516171819202122232425262728[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network.target[Service]Type=notify# the default is not to use systemd for cgroups because the delegate issues still# exists and systemd currently does not support the cgroup feature set required# for containers run by dockerExecStart=/usr/bin/dockerd -H 0.0.0.0:2375 -H unix:///var/run/docker.sock --registry-mirror https://registry.docker-cn.com --insecure-registry 172.16.59.153 --insecure-registry hub.cxwen.cn --insecure-registry k8s.gcr.io --insecure-registry quay.io --default-ulimit core=0:0 --live-restoreExecReload=/bin/kill -s HUP $MAINPID# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity# Uncomment TasksMax if your systemd version supports it.# Only systemd 226 and above support this version.#TasksMax=infinityTimeoutStartSec=0# set delegate yes so that systemd does not reset the cgroups of docker containersDelegate=yes# kill only the docker process, not all processes in the cgroupKillMode=process[Install]WantedBy=multi-user.target 1234--registry-mirror：指定 docker pull 时使用的注册服务器镜像地址,指定为https://registry.docker-cn.com可以加快docker hub中的镜像拉取速度--insecure-registry：配置非安全的docker镜像注册服务器--default-ulimit：配置容器默认的ulimit选项--live-restore：开启此选项，当dockerd服务出现问题时，容器照样运行，服务恢复后，容器也可以再被服务抓到并可管理 kubernetesetcd以172.28.79.11节点为例，其它节点类似： 1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: v1kind: Podmetadata: labels: component: etcd tier: control-plane name: etcd-172.28.79.11 namespace: kube-systemspec: containers: - command: - etcd - --name=infra0 - --initial-advertise-peer-urls=http://172.28.79.11:2380 - --listen-peer-urls=http://172.28.79.11:2380 - --listen-client-urls=http://172.28.79.11:2379,http://127.0.0.1:2379 - --advertise-client-urls=http://172.28.79.11:2379 - --data-dir=/var/lib/etcd - --initial-cluster-token=etcd-cluster-1 - --initial-cluster=infra0=http://172.28.79.11:2380,infra1=http://172.28.79.12:2380,infra2=http://172.28.79.13:2380,infra3=http://172.28.79.14:2380,infra4=http://172.28.79.15:2380 - --initial-cluster-state=new image: k8s.gcr.io/etcd-amd64:3.1.12 livenessProbe: httpGet: host: 127.0.0.1 path: /health port: 2379 scheme: HTTP failureThreshold: 8 initialDelaySeconds: 15 timeoutSeconds: 15 name: etcd volumeMounts: - name: etcd-data mountPath: /var/lib/etcd hostNetwork: true volumes: - hostPath: path: /var/lib/etcd type: DirectoryOrCreate name: etcd-data kubernetes系统组件kubeadm init 启动k8s集群config.yaml配置123456789101112131415161718192021222324252627282930313233apiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationnetworking: podSubnet: 192.168.0.0/16api: advertiseAddress: 172.28.79.11etcd: endpoints: - http://172.28.79.11:2379 - http://172.28.79.12:2379 - http://172.28.79.13:2379 - http://172.28.79.14:2379 - http://172.28.79.15:2379apiServerCertSANs: - 172.28.79.11 - master01.bja.paas - 172.28.79.12 - master02.bja.paas - 172.28.79.13 - master03.bja.paas - 172.28.79.10 - 127.0.0.1token:kubernetesVersion: v1.10.0apiServerExtraArgs: endpoint-reconciler-type: lease bind-address: 172.28.79.11 runtime-config: storage.k8s.io/v1alpha1=true admission-control: NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuotafeatureGates: CoreDNS: true kubelet配置/etc/systemd/system/kubelet.service.d/10-kubeadm.conf 1234567891011[Service]Environment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\"Environment=\"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true\"Environment=\"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin\"Environment=\"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local\"Environment=\"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt\"Environment=\"KUBELET_CADVISOR_ARGS=--cadvisor-port=0\"Environment=\"KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs\"Environment=\"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki --eviction-hard=memory.available&lt;5%,nodefs.available&lt;5%,imagefs.available&lt;5%\"ExecStart=ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CGROUP_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS keepalivedkeepalived采取直接在物理机部署，使用yum命令进行安装，并设置开机自启。 yum install -y keepalivedsystemctl enable keeplalived 配置keepalived配置文件启动配置文件：”/etc/keepalived/keepalived.conf”。keepalived的MASTER和BACKUP配置有部分差异 MASTER 123456789101112131415161718192021222324252627282930313233! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; root@localhost &#125; router_id master02&#125;vrrp_script chk_haproxy &#123; script \"/etc/keepalived/haproxy_check.sh\" interval 3 weight -20&#125;vrrp_instance VI_1 &#123; state MASTER # BACKUP节点改成BACKUP interface bond1 virtual_router_id 151 priority 110 # BACKUP节点改成100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 172.28.79.10 # k8s使用的VIP 172.28.79.9 # 数据库组件使用的VIP &#125; track_script &#123; chk_haproxy &#125;&#125; haproxy检查脚本：/etc/keepalived/haproxy_check.sh 123456789#!/bin/bashif [ `ps -C haproxy --no-header |wc -l` -eq 0 ] ; then docker restart k8s-haproxy sleep 2 if [ `ps -C haproxy --no-header |wc -l` -eq 0 ] ; then service keepalived stop fifi haproxyhaproxy以容器的形式启动，启动命令如下： 1docker run -d --net host --restart always --name k8s-haproxy -v /etc/haproxy:/usr/local/etc/haproxy:ro hub.xfyun.cn/k8s/haproxy:1.7 haproxy配置文件：/etc/haproxy/haproxy.cfg 12345678910111213141516171819202122232425global daemon log 127.0.0.1 local0 log 127.0.0.1 local1 notice maxconn 4096defaults log global retries 3 maxconn 2000 timeout connect 5s timeout client 50s timeout server 50sfrontend k8s bind *:6444 mode tcp default_backend k8s-backendbackend k8s-backend balance roundrobin mode tcp server k8s-1 172.28.79.11:6443 check server k8s-2 172.28.79.12:6443 check server k8s-3 172.28.79.13:6443 check 部署完成后操作修改kube-proxy configmap1kubectl edit configmap kube-proxy -n kube-system 123456789101112131415161718192021.....kubeconfig.conf: |- apiVersion: v1 kind: Config clusters: - cluster: certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt server: https://172.28.79.10:6444 # 更改此行ip为vip,改成172.28.79.10 name: default contexts: - context: cluster: default namespace: default user: default name: default current-context: default users: - name: default user: tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token...... 执行如下命令让kube-proxy组件重新启动 1kubectl get pod -n kube-system | grep kube-proxy | awk '&#123;print $1&#125;' | xargs kubectl delete pod -n kube-system 修改所有node节点kubelet.conf1/etc/kubernetes/kubelet.conf 1234567891011121314151617181920apiVersion: v1clusters:- cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1EVXhPREF4TXpNME1Gb1hEVEk0TURVeE5UQXhNek0wTUZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTGJoCmw1TDRaNHFiWTJ3MmY5TFlEb0ZqVlhhcHRhYklkQmZmTS9zMTJaWFd1NU5LYWlPR09ub3RxK1gwM0VJb3Z4VEkKUGh5NzBqY294VGlLUTk5ZkFsUS82a2Vhc0x5MDNGZXJvYkhmaldUenBkZE5mWVNEZStMazlMV0hIZ0phOXVUQQpDU3kyay9sZGo3VWQ0Sk9pMi9lcGhVTUNNMUNlbmdPeWZDNUl0SUpFZzJmMk95cTE5U0JBeW1zYzFTalg5Q0F6CnNyMlhiTm9hK1lVS2Flek1QSldvYlNxdEg0czQ1TkluYytMREJFTkk4VGVITktybENsamdIeUorUjU1V2pCTW8KeSs3Y1BxL2cwTkxmSU4xRjJVbkFFa3RTSmVYUFBSaGlQUUhJcGRBU0xySXhVcE9HNlN3Yk51bmRGdGsxaUJiUgpUSW9md2UyT0VhZkhySmV5OHJrQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLME1mOFM5VjUyaG9VZ3JYcGQyK09rOHF2Ny8KR3hpWnRFelFISW9RdWRLLzJ2ZGJHbXdnem10V3hFNHRYRWQyUnlXTTZZR1VQSmNpMmszY1Z6QkpSaGcvWFB2UQppRVBpUDk5ZkdiM0kxd0QyanlURWVaZVd1ekdSRDk5ait3bStmcE9wQzB2ZU1LN3hzM1VURjRFOFlhWGcwNmdDCjBXTkFNdTRxQmZaSUlKSEVDVDhLUlB5TEN5Zlgvbm84Q25WTndaM3pCbGZaQmFONGZaOWw0UUdGMVd4dlc0OHkKYmpvRDhqUVJnL1kwYUVUMWMrSEhpWTNmNDF0dG9kMWJoSWR3c1NDNUhhRjJQSVAvZ2dCSnZ2Uzh2V1cwcVRDegpDV2EzcVJ0bVB0MHdtcEZic2RPWmdsWkl6aWduYTdaaDFWMDJVM0VFZ2kwYjNGZWR5OW5MRUZaMGJZbz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= server: https://172.28.79.10:6444 # 此处改为VIP加haproxy监听端口6444 name: default-clustercontexts:- context: cluster: default-cluster namespace: default user: default-auth name: default-contextcurrent-context: default-contextkind: Configpreferences: &#123;&#125;users:- name: default-auth user: client-certificate: /var/lib/kubelet/pki/kubelet-client.crt client-key: /var/lib/kubelet/pki/kubelet-client.key 部署前注意事项1. 确保所有节点时间同步使用ntpdate命令进行时间同步，若无私网时间服务器，可以使用阿里云时间服务器。 1ntpdate ntp1.aliyun.com 2. 确保所有节点ip转发功能打开1net.ipv4.ip_forward = 1","categories":[{"name":"kubernetes及容器","slug":"kubernetes及容器","permalink":"http://yoursite.com/categories/kubernetes及容器/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"},{"name":"容器","slug":"容器","permalink":"http://yoursite.com/tags/容器/"},{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-06-20T11:49:39.594Z","updated":"2019-06-20T11:49:39.594Z","comments":true,"path":"2019/06/20/hello-world/","link":"","permalink":"http://yoursite.com/2019/06/20/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}